{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import piexif\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import entropy\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_exts = (\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\")\n",
    "\n",
    "def extract_exif_info(image_path, img_format):\n",
    "   if img_format not in [\"JPEG\", \"TIFF\"]:\n",
    "       return {\"Make\": None, \"Model\": None, \"Software\": None, \"DateTimeOriginal\": None}\n",
    "   try:\n",
    "       exif_dict = piexif.load(image_path)\n",
    "       exif_data = {}\n",
    "       for ifd in exif_dict:\n",
    "           for tag in exif_dict[ifd]:\n",
    "               key = piexif.TAGS[ifd][tag][\"name\"]\n",
    "               value = exif_dict[ifd][tag]\n",
    "               if isinstance(value, bytes):\n",
    "                   value = value.decode(errors=\"ignore\")\n",
    "               exif_data[key] = value\n",
    "       return {\n",
    "           \"Make\": exif_data.get(\"Make\", None),\n",
    "           \"Model\": exif_data.get(\"Model\", None),\n",
    "           \"Software\": exif_data.get(\"Software\", None),\n",
    "           \"DateTimeOriginal\": exif_data.get(\"DateTimeOriginal\", None)\n",
    "       }\n",
    "   except:\n",
    "       return {\"Make\": None, \"Model\": None, \"Software\": None, \"DateTimeOriginal\": None}\n",
    "\n",
    "def extract_qtable(image_path, img_format):\n",
    "   if img_format != \"JPEG\":\n",
    "       return {\"qtable_mean\": None, \"qtable_std\": None, \"qtable_max\": None, \"qtable_min\": None}\n",
    "   try:\n",
    "       img = Image.open(image_path)\n",
    "       qt = img.quantization\n",
    "       if not qt:\n",
    "           return {\"qtable_mean\": None, \"qtable_std\": None}\n",
    "       q = list(qt.values())[0]\n",
    "       q = np.array(q)\n",
    "       return {\n",
    "           \"qtable_mean\": np.mean(q),\n",
    "           \"qtable_std\": np.std(q),\n",
    "           \"qtable_max\": np.max(q),\n",
    "           \"qtable_min\": np.min(q)\n",
    "       }\n",
    "   except:\n",
    "       return {\"qtable_mean\": None, \"qtable_std\": None, \"qtable_max\": None, \"qtable_min\": None}\n",
    "\n",
    "def extract_entropy(img_array):\n",
    "   r, g, b = img_array[:, :, 0], img_array[:, :, 1], img_array[:, :, 2]\n",
    "   return {\n",
    "       \"r_entropy\": entropy(np.histogram(r, bins=256)[0] + 1e-7),\n",
    "       \"g_entropy\": entropy(np.histogram(g, bins=256)[0] + 1e-7),\n",
    "       \"b_entropy\": entropy(np.histogram(b, bins=256)[0] + 1e-7)\n",
    "   }\n",
    "\n",
    "def extract_blur_sharpness(img_array):\n",
    "   gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n",
    "   lap_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "   return {\"blur_metric\": lap_var}\n",
    "\n",
    "def extract_hu_moments(img_array):\n",
    "   gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n",
    "   moments = cv2.moments(gray)\n",
    "   hu_moments = cv2.HuMoments(moments).flatten()\n",
    "   return {f\"hu_{i+1}\": float(val) for i, val in enumerate(hu_moments)}\n",
    "\n",
    "def extract_dct_features(img_array):\n",
    "    gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n",
    "    h, w = gray.shape\n",
    "    # Crop to even dimensions if necessary\n",
    "    if h % 2 != 0:\n",
    "        gray = gray[:-1, :]\n",
    "    if w % 2 != 0:\n",
    "        gray = gray[:, :-1]\n",
    "    dct = cv2.dct(np.float32(gray) / 255.0)\n",
    "    dct_abs = np.abs(dct)\n",
    "    return {\n",
    "        \"dct_mean\": np.mean(dct_abs),\n",
    "        \"dct_std\": np.std(dct_abs),\n",
    "        \"dct_max\": np.max(dct_abs),\n",
    "        \"dct_energy\": np.sum(dct_abs ** 2)\n",
    "    }\n",
    "\n",
    "def extract_noise_features(img_array):\n",
    "   gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n",
    "   blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "   noise = gray.astype(np.float32) - blurred.astype(np.float32)\n",
    "   return {\n",
    "       \"noise_mean\": np.mean(noise),\n",
    "       \"noise_std\": np.std(noise)\n",
    "   }\n",
    "\n",
    "def extract_edge_density(img_array):\n",
    "   gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n",
    "   edges = cv2.Canny(gray, 100, 200)\n",
    "   return {\n",
    "       \"edge_density\": np.sum(edges > 0) / edges.size\n",
    "   }\n",
    "\n",
    "def extract_image_stats(img):\n",
    "   return {\n",
    "       \"width\": img.width,\n",
    "       \"height\": img.height\n",
    "   }\n",
    "\n",
    "def process_images(folder, label):\n",
    "   data = []\n",
    "   for fname in tqdm(os.listdir(folder), desc=f\"Processing {folder}\"):\n",
    "       if not fname.lower().endswith(valid_exts):\n",
    "           continue\n",
    "       full_path = os.path.join(folder, fname)\n",
    "       try:\n",
    "           img = Image.open(full_path).convert(\"RGB\")\n",
    "           img_array = np.array(img)\n",
    "           img_format = img.format or os.path.splitext(fname)[-1].replace(\".\", \"\").upper()\n",
    "\n",
    "           row = {\n",
    "               \"filename\": fname,\n",
    "               \"label\": label,\n",
    "               \"format\": img_format,\n",
    "               \"has_exif\": img_format in ['JPEG', 'TIFF'],\n",
    "               \"has_qtable\": img_format == 'JPEG'\n",
    "           }\n",
    "\n",
    "           row.update(extract_exif_info(full_path, img_format))\n",
    "           row.update(extract_image_stats(img))\n",
    "           row.update(extract_entropy(img_array))\n",
    "           row.update(extract_blur_sharpness(img_array))\n",
    "           row.update(extract_hu_moments(img_array))\n",
    "           row.update(extract_dct_features(img_array))\n",
    "           row.update(extract_noise_features(img_array))\n",
    "           row.update(extract_edge_density(img_array))\n",
    "           row.update(extract_qtable(full_path, img_format))\n",
    "\n",
    "           data.append(row)\n",
    "       except Exception as e:\n",
    "           print(f\"[ERROR] {fname}: {e}\")\n",
    "   return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing human: 100%|██████████| 39975/39975 [15:14<00:00, 43.71it/s]\n",
      "Processing edited: 100%|██████████| 5124/5124 [00:54<00:00, 94.40it/s] \n",
      "Processing AI: 100%|██████████| 39975/39975 [14:52<00:00, 44.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata extraction complete. Saved to image_detection_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "# Folder to label mapping\n",
    "folder_label_map = {\n",
    "    'human': 0,\n",
    "    'edited': 1,\n",
    "    'AI': 2\n",
    "}\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for folder, label in folder_label_map.items():\n",
    "    folder_path = f'{folder}'\n",
    "    data = process_images(folder_path, label)\n",
    "    all_data.extend(data)\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv('image_detection_metadata.csv', index=False)\n",
    "print('Metadata extraction complete. Saved to image_detection_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/wei/Downloads/NLP/image_detection\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set working directory temporarily for this notebook session\n",
    "os.chdir('/Users/wei/Downloads/NLP/image_detection')\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('image_detection_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For stage 1: real (0) vs not real (1)\n",
    "df_stage1 = df.copy()\n",
    "df_stage1['label_stage1'] = df_stage1['label'].apply(lambda x: 0 if x == 0 else 1)\n",
    "df_stage1.to_csv('train_meta_stage1.csv', index=False)\n",
    "# For stage 2: only edited (1) and AI (2)\n",
    "df_stage2 = df[df['label'] != 0].copy()\n",
    "df_stage2['label_stage2'] = df_stage2['label'].apply(lambda x: 0 if x == 1 else 1)\n",
    "df_stage2.to_csv('train_meta_stage2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wei/miniforge3/envs/nlp_env/lib/python3.10/site-packages/sklearn/utils/extmath.py:1144: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/Users/wei/miniforge3/envs/nlp_env/lib/python3.10/site-packages/sklearn/utils/extmath.py:1149: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/Users/wei/miniforge3/envs/nlp_env/lib/python3.10/site-packages/sklearn/utils/extmath.py:1169: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/Users/wei/miniforge3/envs/nlp_env/lib/python3.10/site-packages/sklearn/utils/extmath.py:1144: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/Users/wei/miniforge3/envs/nlp_env/lib/python3.10/site-packages/sklearn/utils/extmath.py:1149: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/Users/wei/miniforge3/envs/nlp_env/lib/python3.10/site-packages/sklearn/utils/extmath.py:1169: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load stage 1 data\n",
    "df1 = pd.read_csv('train_meta_stage1.csv')\n",
    "\n",
    "# Stratified split: 60% train, 20% val, 20% test\n",
    "trainval_df1, test_df1 = train_test_split(df1, test_size=0.2, stratify=df1['label_stage1'], random_state=42)\n",
    "train_df1, val_df1 = train_test_split(trainval_df1, test_size=0.25, stratify=trainval_df1['label_stage1'], random_state=42)\n",
    "\n",
    "\n",
    "# Example for stage 1\n",
    "meta_features1 = [col for col in train_df1.columns if col not in ['filename', 'label', 'format', 'label_stage1', 'folder']]\n",
    "\n",
    "scaler1 = StandardScaler()\n",
    "train_df1[meta_features1] = scaler1.fit_transform(train_df1[meta_features1])\n",
    "val_df1[meta_features1] = scaler1.transform(val_df1[meta_features1])\n",
    "test_df1[meta_features1] = scaler1.transform(test_df1[meta_features1])\n",
    "\n",
    "# Save scaler1 for later use\n",
    "import joblib\n",
    "joblib.dump(scaler1, 'metadata_scaler_stage1.pkl')\n",
    "# Save splits\n",
    "train_df1.to_csv('train_meta_stage1_train.csv', index=False)\n",
    "val_df1.to_csv('train_meta_stage1_val.csv', index=False)\n",
    "test_df1.to_csv('train_meta_stage1_test.csv', index=False)\n",
    "\n",
    "# Load stage 2 data\n",
    "df2 = pd.read_csv('train_meta_stage2.csv')\n",
    "\n",
    "# Stratified split: 60% train, 20% val, 20% test\n",
    "trainval_df2, test_df2 = train_test_split(df2, test_size=0.2, stratify=df2['label_stage2'], random_state=42)\n",
    "train_df2, val_df2 = train_test_split(trainval_df2, test_size=0.25, stratify=trainval_df2['label_stage2'], random_state=42)\n",
    "meta_features2 = [col for col in train_df2.columns if col not in ['filename', 'label', 'format', 'label_stage2', 'folder']]\n",
    "\n",
    "scaler2 = StandardScaler()\n",
    "train_df2[meta_features2] = scaler2.fit_transform(train_df2[meta_features2])\n",
    "val_df2[meta_features2] = scaler2.transform(val_df2[meta_features2])\n",
    "test_df2[meta_features2] = scaler2.transform(test_df2[meta_features2])\n",
    "\n",
    "# Save scaler1 for later use\n",
    "import joblib\n",
    "joblib.dump(scaler2, 'metadata_scaler_stage2.pkl')\n",
    "# Save splits\n",
    "train_df2.to_csv('train_meta_stage2_train.csv', index=False)\n",
    "val_df2.to_csv('train_meta_stage2_val.csv', index=False)\n",
    "test_df2.to_csv('train_meta_stage2_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load your stage 1 CSVs\n",
    "# train_df1 = pd.read_csv('train_meta_stage1_train.csv')\n",
    "# val_df1 = pd.read_csv('train_meta_stage1_val.csv')\n",
    "# test_df1 = pd.read_csv('train_meta_stage1_test.csv')\n",
    "\n",
    "# Add folder column based on original label\n",
    "def get_folder(label):\n",
    "    if label == 0:\n",
    "        return 'human'\n",
    "    elif label == 1:\n",
    "        return 'edited'\n",
    "    else:\n",
    "        return 'AI'\n",
    "\n",
    "train_df1['folder'] = train_df1['label'].apply(get_folder)\n",
    "val_df1['folder'] = val_df1['label'].apply(get_folder)\n",
    "test_df1['folder'] = test_df1['label'].apply(get_folder)\n",
    "\n",
    "# Save updated CSVs\n",
    "train_df1.to_csv('train_meta_stage1_train.csv', index=False)\n",
    "val_df1.to_csv('train_meta_stage1_val.csv', index=False)\n",
    "test_df1.to_csv('train_meta_stage1_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df2 = pd.read_csv('train_meta_stage2_train.csv')\n",
    "# val_df2 = pd.read_csv('train_meta_stage2_val.csv')\n",
    "# test_df2 = pd.read_csv('train_meta_stage2_test.csv')\n",
    "\n",
    "# Add folder column based on original label\n",
    "def get_folder(label):\n",
    "    if label == 0:\n",
    "        return 'human'\n",
    "    elif label == 1:\n",
    "        return 'edited'\n",
    "    else:\n",
    "        return 'AI'\n",
    "\n",
    "train_df2['folder'] = train_df2['label'].apply(get_folder)\n",
    "val_df2['folder'] = val_df2['label'].apply(get_folder)\n",
    "test_df2['folder'] = test_df2['label'].apply(get_folder)\n",
    "\n",
    "# Save updated CSVs\n",
    "train_df2.to_csv('image_detection/train_meta_stage2_train.csv', index=False)\n",
    "val_df2.to_csv('image_detection/train_meta_stage2_val.csv', index=False)\n",
    "test_df2.to_csv('image_detection/train_meta_stage2_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/wei/Downloads/NLP\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/wei/Downloads/NLP\n",
    "from image_detection.image_metadata_utils import extract_metadata_from_pil_image, HybridNet, transform, HybridImageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "labels = train_df2['label_stage2'].astype(int).values\n",
    "class_sample_counts = np.bincount(labels)  # [num_class_0, num_class_1]\n",
    "weights = 1. / class_sample_counts         # inverse frequency\n",
    "sample_weights = weights[labels]\n",
    "\n",
    "# Use weighted sampler to balance training\n",
    "sampler = WeightedRandomSampler(sample_weights,\n",
    "                                num_samples=len(sample_weights),\n",
    "                                replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/wei/Downloads/NLP\n",
      "Using MPS device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wei/miniforge3/envs/nlp_env/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/wei/miniforge3/envs/nlp_env/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "%cd /Users/wei/Downloads/NLP\n",
    "\n",
    "# --- Stage 1: real vs not real ---\n",
    "meta_features1 = [col for col in pd.read_csv('image_detection/train_meta_stage1_train.csv').columns if col not in ['filename', 'label', 'format', 'label_stage1', 'folder']]\n",
    "train_dataset1 = HybridImageDataset('image_detection', 'image_detection/train_meta_stage1_train.csv', transform, meta_features1, label_col='label_stage1')\n",
    "val_dataset1   = HybridImageDataset('image_detection', 'image_detection/train_meta_stage1_val.csv', transform, meta_features1, label_col='label_stage1')\n",
    "train_loader1 = DataLoader(train_dataset1, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader1   = DataLoader(val_dataset1, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "# --- Stage 2: edited vs AI ---\n",
    "meta_features2 = [col for col in pd.read_csv('image_detection/train_meta_stage2_train.csv').columns if col not in ['filename', 'label', 'format', 'label_stage2', 'folder']]\n",
    "train_dataset2 = HybridImageDataset('image_detection', 'image_detection/train_meta_stage2_train.csv', transform, meta_features2, label_col='label_stage2')\n",
    "val_dataset2   = HybridImageDataset('image_detection', 'image_detection/train_meta_stage2_val.csv', transform, meta_features2, label_col='label_stage2')\n",
    "train_loader2 = DataLoader(train_dataset2, batch_size=32, sampler=sampler, num_workers=0)\n",
    "val_loader2   = DataLoader(val_dataset2, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "# --- Model definitions ---\n",
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS device\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU device\")\n",
    "# model1 = HybridNet(num_metadata_features=len(meta_features1), num_classes=2).to(device)\n",
    "model2 = HybridNet(num_metadata_features=len(meta_features2), num_classes=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [Train]: 100%|██████████| 846/846 [07:04<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train Loss: 0.0562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [Val]: 100%|██████████| 282/282 [01:04<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Val Loss: 0.0128 | Val Acc: 0.9963\n",
      "Validation loss improved, saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 [Train]: 100%|██████████| 846/846 [07:03<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Train Loss: 0.0072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 [Val]: 100%|██████████| 282/282 [01:03<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Val Loss: 0.0077 | Val Acc: 0.9979\n",
      "Validation loss improved, saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 [Train]: 100%|██████████| 846/846 [07:01<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Train Loss: 0.0054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 [Val]: 100%|██████████| 282/282 [01:04<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Val Loss: 0.0103 | Val Acc: 0.9976\n",
      "No improvement for 1 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 [Train]: 100%|██████████| 846/846 [07:10<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Train Loss: 0.0025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 [Val]: 100%|██████████| 282/282 [01:04<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Val Loss: 0.0091 | Val Acc: 0.9976\n",
      "No improvement for 2 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 [Train]: 100%|██████████| 846/846 [07:09<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Train Loss: 0.0033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 [Val]: 100%|██████████| 282/282 [01:04<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Val Loss: 0.0122 | Val Acc: 0.9971\n",
      "No improvement for 3 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 [Train]: 100%|██████████| 846/846 [07:26<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Train Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 [Val]: 100%|██████████| 282/282 [01:02<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Val Loss: 0.0103 | Val Acc: 0.9977\n",
      "No improvement for 4 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 [Train]: 100%|██████████| 846/846 [07:51<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Train Loss: 0.0026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 [Val]: 100%|██████████| 282/282 [15:55<00:00,  3.39s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Val Loss: 0.0068 | Val Acc: 0.9983\n",
      "Validation loss improved, saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 [Train]: 100%|██████████| 846/846 [1:51:45<00:00,  7.93s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Train Loss: 0.0026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 [Val]: 100%|██████████| 282/282 [16:47<00:00,  3.57s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Val Loss: 0.0107 | Val Acc: 0.9977\n",
      "No improvement for 1 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 [Train]: 100%|██████████| 846/846 [1:54:08<00:00,  8.10s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Train Loss: 0.0038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 [Val]: 100%|██████████| 282/282 [08:42<00:00,  1.85s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Val Loss: 0.0110 | Val Acc: 0.9971\n",
      "No improvement for 2 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 [Train]: 100%|██████████| 846/846 [54:06<00:00,  3.84s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Train Loss: 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 [Val]: 100%|██████████| 282/282 [00:54<00:00,  5.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Val Loss: 0.0079 | Val Acc: 0.9979\n",
      "No improvement for 3 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 [Train]: 100%|██████████| 846/846 [42:36<00:00,  3.02s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Train Loss: 0.0035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 [Val]: 100%|██████████| 282/282 [00:54<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Val Loss: 0.0078 | Val Acc: 0.9979\n",
      "No improvement for 4 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 [Train]: 100%|██████████| 846/846 [1:38:57<00:00,  7.02s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Train Loss: 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 [Val]: 100%|██████████| 282/282 [16:52<00:00,  3.59s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Val Loss: 0.0070 | Val Acc: 0.9983\n",
      "No improvement for 5 epoch(s).\n",
      "Early stopping triggered after 12 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "def train_model(model, train_loader, val_loader, device, num_epochs=20, lr=1e-4, patience=5, save_path='best_model.pth', criterion=None):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    if criterion is None:\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for images, metas, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
    "            images, metas, labels = images.to(device), metas.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, metas)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1} Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for images, metas, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\"):\n",
    "                images, metas, labels = images.to(device), metas.to(device), labels.to(device)\n",
    "                outputs = model(images, metas)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                preds = outputs.argmax(1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = correct / len(val_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1} Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "            torch.save(best_model_state, save_path)\n",
    "            print(\"Validation loss improved, saving model.\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"No improvement for {epochs_no_improve} epoch(s).\")\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "                break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    return model\n",
    "\n",
    "\n",
    "# --- Train model2 (with class weighting) ---\n",
    "\n",
    "model2 = train_model(model2, train_loader2, val_loader2, device, num_epochs=20, lr=1e-4, patience=5, save_path='hybrid_stage2.pth'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [Train]:   0%|          | 0/1596 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [Train]: 100%|██████████| 1596/1596 [27:22<00:00,  1.03s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train Loss: 0.1371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [Val]: 100%|██████████| 532/532 [01:56<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Val Loss: 0.0675 | Val Acc: 0.9762\n",
      "Validation loss improved, saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 [Train]: 100%|██████████| 1596/1596 [34:52<00:00,  1.31s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Train Loss: 0.0618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 [Val]: 100%|██████████| 532/532 [01:54<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Val Loss: 0.0577 | Val Acc: 0.9794\n",
      "Validation loss improved, saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 [Train]: 100%|██████████| 1596/1596 [55:07<00:00,  2.07s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Train Loss: 0.0396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 [Val]: 100%|██████████| 532/532 [02:10<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Val Loss: 0.0604 | Val Acc: 0.9787\n",
      "No improvement for 1 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 [Train]: 100%|██████████| 1596/1596 [24:32<00:00,  1.08it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Train Loss: 0.0283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 [Val]: 100%|██████████| 532/532 [02:03<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Val Loss: 0.0665 | Val Acc: 0.9799\n",
      "No improvement for 2 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 [Train]: 100%|██████████| 1596/1596 [28:50<00:00,  1.08s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Train Loss: 0.0218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 [Val]: 100%|██████████| 532/532 [01:57<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Val Loss: 0.0769 | Val Acc: 0.9778\n",
      "No improvement for 3 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 [Train]: 100%|██████████| 1596/1596 [44:40<00:00,  1.68s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Train Loss: 0.0174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 [Val]: 100%|██████████| 532/532 [01:53<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Val Loss: 0.0579 | Val Acc: 0.9828\n",
      "No improvement for 4 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 [Train]: 100%|██████████| 1596/1596 [14:54<00:00,  1.78it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Train Loss: 0.0172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 [Val]: 100%|██████████| 532/532 [01:44<00:00,  5.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Val Loss: 0.0563 | Val Acc: 0.9826\n",
      "Validation loss improved, saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 [Train]: 100%|██████████| 1596/1596 [13:42<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Train Loss: 0.0131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 [Val]: 100%|██████████| 532/532 [01:51<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Val Loss: 0.0599 | Val Acc: 0.9830\n",
      "No improvement for 1 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 [Train]: 100%|██████████| 1596/1596 [14:07<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Train Loss: 0.0135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 [Val]: 100%|██████████| 532/532 [01:54<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Val Loss: 0.0590 | Val Acc: 0.9846\n",
      "No improvement for 2 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 [Train]: 100%|██████████| 1596/1596 [15:14<00:00,  1.75it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Train Loss: 0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 [Val]: 100%|██████████| 532/532 [01:55<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Val Loss: 0.0558 | Val Acc: 0.9854\n",
      "Validation loss improved, saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 [Train]: 100%|██████████| 1596/1596 [14:23<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Train Loss: 0.0122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 [Val]: 100%|██████████| 532/532 [02:12<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Val Loss: 0.0717 | Val Acc: 0.9848\n",
      "No improvement for 1 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 [Train]: 100%|██████████| 1596/1596 [14:39<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Train Loss: 0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 [Val]: 100%|██████████| 532/532 [02:10<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Val Loss: 0.0654 | Val Acc: 0.9864\n",
      "No improvement for 2 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 [Train]: 100%|██████████| 1596/1596 [14:45<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Train Loss: 0.0078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 [Val]: 100%|██████████| 532/532 [02:10<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Val Loss: 0.0671 | Val Acc: 0.9852\n",
      "No improvement for 3 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 [Train]: 100%|██████████| 1596/1596 [14:36<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Train Loss: 0.0094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 [Val]: 100%|██████████| 532/532 [02:03<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Val Loss: 0.0612 | Val Acc: 0.9861\n",
      "No improvement for 4 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 [Train]: 100%|██████████| 1596/1596 [14:09<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Train Loss: 0.0103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 [Val]: 100%|██████████| 532/532 [01:57<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Val Loss: 0.0672 | Val Acc: 0.9852\n",
      "No improvement for 5 epoch(s).\n",
      "Early stopping triggered after 15 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [Train]: 100%|██████████| 846/846 [07:32<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train Loss: 0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [Val]: 100%|██████████| 282/282 [01:01<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Val Loss: 0.0003 | Val Acc: 0.8864\n",
      "Validation loss improved, saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 [Train]: 100%|██████████| 846/846 [21:43<00:00,  1.54s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Train Loss: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 [Val]: 100%|██████████| 282/282 [01:04<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Val Loss: 0.0001 | Val Acc: 0.8864\n",
      "Validation loss improved, saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 [Train]: 100%|██████████| 846/846 [07:34<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Train Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 [Val]: 100%|██████████| 282/282 [01:04<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Val Loss: 0.0000 | Val Acc: 0.8864\n",
      "Validation loss improved, saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 [Train]: 100%|██████████| 846/846 [07:28<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Train Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 [Val]: 100%|██████████| 282/282 [01:13<00:00,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Val Loss: 0.0000 | Val Acc: 0.8864\n",
      "Validation loss improved, saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 [Train]: 100%|██████████| 846/846 [07:26<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Train Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 [Val]: 100%|██████████| 282/282 [01:01<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Val Loss: 0.0000 | Val Acc: 0.8864\n",
      "Validation loss improved, saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 [Train]: 100%|██████████| 846/846 [1:18:07<00:00,  5.54s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Train Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 [Val]: 100%|██████████| 282/282 [22:25<00:00,  4.77s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Val Loss: 0.0000 | Val Acc: 0.8864\n",
      "Validation loss improved, saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 [Train]: 100%|██████████| 846/846 [1:58:31<00:00,  8.41s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Train Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 [Val]: 100%|██████████| 282/282 [16:10<00:00,  3.44s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Val Loss: 0.0000 | Val Acc: 0.8864\n",
      "Validation loss improved, saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 [Train]: 100%|██████████| 846/846 [2:32:36<00:00, 10.82s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Train Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 [Val]: 100%|██████████| 282/282 [16:09<00:00,  3.44s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Val Loss: 0.0000 | Val Acc: 0.8864\n",
      "Validation loss improved, saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 [Train]: 100%|██████████| 846/846 [34:27<00:00,  2.44s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Train Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 [Val]: 100%|██████████| 282/282 [00:55<00:00,  5.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Val Loss: 0.0000 | Val Acc: 0.8864\n",
      "Validation loss improved, saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 [Train]: 100%|██████████| 846/846 [1:08:03<00:00,  4.83s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Train Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 [Val]: 100%|██████████| 282/282 [16:45<00:00,  3.57s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Val Loss: 0.0000 | Val Acc: 0.8864\n",
      "Validation loss improved, saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 [Train]: 100%|██████████| 846/846 [23:56<00:00,  1.70s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Train Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 [Val]: 100%|██████████| 282/282 [00:53<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Val Loss: 0.0000 | Val Acc: 0.8864\n",
      "Validation loss improved, saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 [Train]: 100%|██████████| 846/846 [06:58<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Train Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 [Val]: 100%|██████████| 282/282 [00:51<00:00,  5.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Val Loss: 0.0000 | Val Acc: 0.8864\n",
      "Validation loss improved, saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 [Train]: 100%|██████████| 846/846 [07:01<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Train Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 [Val]: 100%|██████████| 282/282 [00:53<00:00,  5.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Val Loss: 0.0000 | Val Acc: 0.8864\n",
      "Validation loss improved, saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 [Train]: 100%|██████████| 846/846 [07:03<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Train Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 [Val]: 100%|██████████| 282/282 [00:53<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Val Loss: 0.0000 | Val Acc: 0.8864\n",
      "Validation loss improved, saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 [Train]: 100%|██████████| 846/846 [07:01<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Train Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 [Val]: 100%|██████████| 282/282 [00:52<00:00,  5.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Val Loss: 0.0000 | Val Acc: 0.8864\n",
      "Validation loss improved, saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 [Train]: 100%|██████████| 846/846 [07:01<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Train Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 [Val]: 100%|██████████| 282/282 [00:52<00:00,  5.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Val Loss: 0.0000 | Val Acc: 0.8864\n",
      "Validation loss improved, saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 [Train]: 100%|██████████| 846/846 [07:03<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Train Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 [Val]: 100%|██████████| 282/282 [00:53<00:00,  5.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Val Loss: 0.0000 | Val Acc: 0.8864\n",
      "Validation loss improved, saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 [Train]: 100%|██████████| 846/846 [07:04<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Train Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 [Val]: 100%|██████████| 282/282 [00:55<00:00,  5.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Val Loss: 0.0000 | Val Acc: 0.8864\n",
      "Validation loss improved, saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 [Train]: 100%|██████████| 846/846 [15:21<00:00,  1.09s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Train Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 [Val]: 100%|██████████| 282/282 [00:57<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Val Loss: 0.0000 | Val Acc: 0.8864\n",
      "Validation loss improved, saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 [Train]: 100%|██████████| 846/846 [07:09<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Train Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 [Val]: 100%|██████████| 282/282 [00:57<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Val Loss: 0.0000 | Val Acc: 0.8864\n",
      "Validation loss improved, saving model.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "# --- Training function (from previous message) ---\n",
    "def train_model(model, train_loader, val_loader, device, num_epochs=20, lr=1e-4, patience=5, save_path='best_model.pth'):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "    if criterion is None:\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for images, metas, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
    "            images, metas, labels = images.to(device), metas.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, metas)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1} Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for images, metas, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\"):\n",
    "                images, metas, labels = images.to(device), metas.to(device), labels.to(device)\n",
    "                outputs = model(images, metas)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                preds = outputs.argmax(1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = correct / len(val_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1} Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "            torch.save(best_model_state, save_path)\n",
    "            print(\"Validation loss improved, saving model.\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"No improvement for {epochs_no_improve} epoch(s).\")\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "                break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    return model\n",
    "# --- Train both models ---\n",
    "model1 = train_model(model1, train_loader1, val_loader1, device, num_epochs=20, lr=1e-4, patience=5, save_path='hybrid_stage1.pth')\n",
    "model2 = train_model(model2, train_loader2, val_loader2, device, num_epochs=20, lr=1e-4, patience=5, save_path='hybrid_stage2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "import json\n",
    "import torch\n",
    "\n",
    "def optimize_thresholds(model, val_loader, device, num_classes, out_json):\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, metas, labels in tqdm(val_loader, desc=\"Collecting validation predictions\"):\n",
    "            images, metas = images.to(device), metas.to(device)\n",
    "            outputs = model(images, metas)\n",
    "            probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "            all_probs.append(probs)\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    all_probs = np.concatenate(all_probs, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    best_thresholds = [0.5] * num_classes\n",
    "    best_f1s = [0] * num_classes\n",
    "\n",
    "    for class_idx in range(num_classes):\n",
    "        for thresh in np.arange(0.1, 0.9, 0.01):\n",
    "            preds = []\n",
    "            for prob in all_probs:\n",
    "                if prob[class_idx] > thresh:\n",
    "                    preds.append(class_idx)\n",
    "                else:\n",
    "                    preds.append(np.argmax(prob))\n",
    "            f1 = f1_score(all_labels, preds, average='macro')\n",
    "            if f1 > best_f1s[class_idx]:\n",
    "                best_f1s[class_idx] = f1\n",
    "                best_thresholds[class_idx] = thresh\n",
    "\n",
    "    print(f\"Best thresholds: {best_thresholds}\")\n",
    "    print(f\"Best F1s: {best_f1s}\")\n",
    "\n",
    "    # Save thresholds\n",
    "    thresholds_dict = {f\"class_{i}\": float(best_thresholds[i]) for i in range(num_classes)}\n",
    "    with open(out_json, \"w\") as f:\n",
    "        json.dump(thresholds_dict, f)\n",
    "    print(f\"Saved best thresholds to {out_json}\")\n",
    "    return best_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting validation predictions: 100%|██████████| 532/532 [01:56<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best thresholds: [np.float64(0.4899999999999998), np.float64(0.38999999999999985)]\n",
      "Best F1s: [0.9853159415932478, 0.9855494164270131]\n",
      "Saved best thresholds to best_thresholds_model1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting validation predictions: 100%|██████████| 282/282 [01:01<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best thresholds: [np.float64(0.1), np.float64(0.1)]\n",
      "Best F1s: [0.46987951807228917, 0.46987951807228917]\n",
      "Saved best thresholds to best_thresholds_model2.json\n"
     ]
    }
   ],
   "source": [
    "# For stage 1 (3 classes: human, edited, ai)\n",
    "best_thresholds1 = optimize_thresholds(model1, val_loader1, device, num_classes=2, out_json=\"best_thresholds_model1.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting validation predictions: 100%|██████████| 282/282 [03:29<00:00,  1.35it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best thresholds: [np.float64(0.3199999999999999), np.float64(0.1)]\n",
      "Best F1s: [0.996154132431603, 0.9964181935849987]\n",
      "Saved best thresholds to image_detection/best_thresholds_model2.json\n"
     ]
    }
   ],
   "source": [
    "# For stage 2 (2 classes: edited, ai)\n",
    "best_thresholds2 = optimize_thresholds(model2, val_loader2, device, num_classes=2, out_json=\"image_detection/best_thresholds_model2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_thresholds(probs, threshold_list):\n",
    "    pred = np.argmax(probs)\n",
    "    for idx, thresh in enumerate(threshold_list):\n",
    "        if thresh is not None and probs[idx] > thresh:\n",
    "            pred = idx\n",
    "            break\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model 1 (real vs not real)\n",
    "torch.save(model1.state_dict(), \"image_detection/model1_real_vs_not_real.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model 2 (edited vs ai)\n",
    "torch.save(model2.state_dict(), \"image_detection/model2_edited_vs_ai.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
